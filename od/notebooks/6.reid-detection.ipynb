{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Clone private repo\n",
        "export GITHUB_USER=kiritowu\n",
        "export GITHUB_TOKEN=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\n",
        "export GITHUB_REPOSITORY=Tien-Cheng/brainhack-til-2023-lazythink\n",
        "git clone https://${GITHUB_USER}:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}\n",
        "cd brainhack-til-2023-lazythink/od"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrx6iHN91Qtt",
        "outputId": "ffe11a66-1cca-4f8a-e4b7-19843f870e6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'brainhack-til-2023-lazythink'...\n",
            "Updating files:   3% (268/8519)\rUpdating files:   4% (341/8519)\rUpdating files:   5% (426/8519)\rUpdating files:   6% (512/8519)\rUpdating files:   7% (597/8519)\rUpdating files:   8% (682/8519)\rUpdating files:   9% (767/8519)\rUpdating files:  10% (852/8519)\rUpdating files:  11% (938/8519)\rUpdating files:  12% (1023/8519)\rUpdating files:  13% (1108/8519)\rUpdating files:  14% (1193/8519)\rUpdating files:  15% (1278/8519)\rUpdating files:  16% (1364/8519)\rUpdating files:  17% (1449/8519)\rUpdating files:  18% (1534/8519)\rUpdating files:  19% (1619/8519)\rUpdating files:  20% (1704/8519)\rUpdating files:  21% (1789/8519)\rUpdating files:  22% (1875/8519)\rUpdating files:  23% (1960/8519)\rUpdating files:  24% (2045/8519)\rUpdating files:  25% (2130/8519)\rUpdating files:  26% (2215/8519)\rUpdating files:  27% (2301/8519)\rUpdating files:  28% (2386/8519)\rUpdating files:  29% (2471/8519)\rUpdating files:  30% (2556/8519)\rUpdating files:  31% (2641/8519)\rUpdating files:  32% (2727/8519)\rUpdating files:  33% (2812/8519)\rUpdating files:  34% (2897/8519)\rUpdating files:  35% (2982/8519)\rUpdating files:  36% (3067/8519)\rUpdating files:  37% (3153/8519)\rUpdating files:  38% (3238/8519)\rUpdating files:  39% (3323/8519)\rUpdating files:  40% (3408/8519)\rUpdating files:  41% (3493/8519)\rUpdating files:  42% (3578/8519)\rUpdating files:  43% (3664/8519)\rUpdating files:  44% (3749/8519)\rUpdating files:  45% (3834/8519)\rUpdating files:  46% (3919/8519)\rUpdating files:  47% (4004/8519)\rUpdating files:  48% (4090/8519)\rUpdating files:  49% (4175/8519)\rUpdating files:  50% (4260/8519)\rUpdating files:  51% (4345/8519)\rUpdating files:  52% (4430/8519)\rUpdating files:  53% (4516/8519)\rUpdating files:  54% (4601/8519)\rUpdating files:  55% (4686/8519)\rUpdating files:  56% (4771/8519)\rUpdating files:  57% (4856/8519)\rUpdating files:  58% (4942/8519)\rUpdating files:  59% (5027/8519)\rUpdating files:  60% (5112/8519)\rUpdating files:  61% (5197/8519)\rUpdating files:  62% (5282/8519)\rUpdating files:  63% (5367/8519)\rUpdating files:  64% (5453/8519)\rUpdating files:  65% (5538/8519)\rUpdating files:  66% (5623/8519)\rUpdating files:  67% (5708/8519)\rUpdating files:  68% (5793/8519)\rUpdating files:  69% (5879/8519)\rUpdating files:  70% (5964/8519)\rUpdating files:  71% (6049/8519)\rUpdating files:  72% (6134/8519)\rUpdating files:  73% (6219/8519)\rUpdating files:  74% (6305/8519)\rUpdating files:  75% (6390/8519)\rUpdating files:  76% (6475/8519)\rUpdating files:  77% (6560/8519)\rUpdating files:  78% (6645/8519)\rUpdating files:  79% (6731/8519)\rUpdating files:  80% (6816/8519)\rUpdating files:  81% (6901/8519)\rUpdating files:  82% (6986/8519)\rUpdating files:  83% (7071/8519)\rUpdating files:  84% (7156/8519)\rUpdating files:  85% (7242/8519)\rUpdating files:  86% (7327/8519)\rUpdating files:  87% (7412/8519)\rUpdating files:  88% (7497/8519)\rUpdating files:  89% (7582/8519)\rUpdating files:  90% (7668/8519)\rUpdating files:  91% (7753/8519)\rUpdating files:  92% (7838/8519)\rUpdating files:  93% (7923/8519)\rUpdating files:  94% (8008/8519)\rUpdating files:  95% (8094/8519)\rUpdating files:  96% (8179/8519)\rUpdating files:  97% (8264/8519)\rUpdating files:  98% (8349/8519)\rUpdating files:  99% (8434/8519)\rUpdating files: 100% (8519/8519)\rUpdating files: 100% (8519/8519), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pip install open-clip-torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eCCT4g42L0I",
        "outputId": "9827f510-2908-4d58-b96a-2debd987d4c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Collecting open-clip-torch\n",
            "  Downloading open_clip_torch-2.20.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.15.2+cu118)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2022.10.31)\n",
            "Collecting ftfy (from open-clip-torch)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (4.65.0)\n",
            "Collecting huggingface-hub (from open-clip-torch)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from open-clip-torch)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4 in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (3.20.3)\n",
            "Collecting timm (from open-clip-torch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->open-clip-torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->open-clip-torch) (16.0.5)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->open-clip-torch) (0.2.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (23.1)\n",
            "Collecting safetensors (from timm->open-clip-torch)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open-clip-torch) (1.3.0)\n",
            "Installing collected packages: sentencepiece, safetensors, install, ftfy, huggingface-hub, timm, open-clip-torch\n",
            "Successfully installed ftfy-6.1.1 huggingface-hub-0.14.1 install-1.3.5 open-clip-torch-2.20.0 safetensors-0.3.1 sentencepiece-0.1.99 timm-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n52OM2vV1cap",
        "outputId": "137359f2-c7b9-4bc0-a766-9238e17db816"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=ZxADMwa5hLlm42qKWqEtpdFFwW11g7&prompt=consent&access_type=offline&code_challenge=h499KkX6LKGaZQ9Jmidb0kPnKXKPhHySexEZ1TxO2I0&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AbUR2VNmxPsB1ne9vCZ-F9zxXKi9KZizNpUKqVx5AL-dNzxwHHCMPtoFu09cgp7vXSxjhA\n",
            "\n",
            "You are now logged in as [wongzhaowu@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/Downloads\n",
        "!gsutil cp gs://cloud-ai-platform-e8edc327-855c-4911-bb8e-205517f8c899/cv/suspects.zip  ~/Downloads/suspects.zip \n",
        "!gsutil cp gs://cloud-ai-platform-e8edc327-855c-4911-bb8e-205517f8c899/cv/test-detected.zip  ~/Downloads/test-detected.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO44UbZc1g0q",
        "outputId": "c6200675-3661-487f-f966-f86e69c38057"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copying gs://cloud-ai-platform-e8edc327-855c-4911-bb8e-205517f8c899/cv/suspects.zip...\n",
            "/ [1 files][ 40.1 MiB/ 40.1 MiB]                                                \n",
            "Operation completed over 1 objects/40.1 MiB.                                     \n",
            "Copying gs://cloud-ai-platform-e8edc327-855c-4911-bb8e-205517f8c899/cv/test-detected.zip...\n",
            "\\ [1 files][125.2 MiB/125.2 MiB]                                                \n",
            "Operation completed over 1 objects/125.2 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/brainhack-til-2023-lazythink/od"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlJciz251vNC",
        "outputId": "c9017a3e-56ef-4ff0-c3ea-d241b42a25f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brainhack-til-2023-lazythink/od\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data/images\n",
        "!unzip -q -o ~/Downloads/suspects.zip -d data/images/suspects\n",
        "\n",
        "!mkdir output\n",
        "!unzip -q -o ~/Downloads/test-detected.zip -d test-detected"
      ],
      "metadata": {
        "id": "ybLn4GqH16uw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Obr3nPpG1Nu7",
        "outputId": "5d2db2e7-f200-4b7f-a3f8-10095ee18fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model\n",
            "Downloading (…)ip_pytorch_model.bin: 100% 3.94G/3.94G [00:28<00:00, 137MB/s]\n",
            "num image found: 1600\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/brainhack-til-2023-lazythink/od/../reid/main.py\", line 122, in <module>\n",
            "    main()\n",
            "  File \"/content/brainhack-til-2023-lazythink/od/../reid/main.py\", line 97, in main\n",
            "    suspect_vectors = encode_image(\n",
            "  File \"/content/brainhack-til-2023-lazythink/od/../reid/main.py\", line 66, in encode_image\n",
            "    vectors = model.encode_image(preprocesed_img)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/open_clip/model.py\", line 225, in encode_image\n",
            "    features = self.visual(image)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/open_clip/transformer.py\", line 486, in forward\n",
            "    x = self.transformer(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/open_clip/transformer.py\", line 321, in forward\n",
            "    x = r(x, attn_mask=attn_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/open_clip/transformer.py\", line 242, in forward\n",
            "    x = q_x + self.ls_1(self.attention(q_x=self.ln_1(q_x), k_x=k_x, v_x=v_x, attn_mask=attn_mask))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/open_clip/transformer.py\", line 228, in attention\n",
            "    return self.attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
            "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 5373, in multi_head_attention_forward\n",
            "    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1004.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 224.81 MiB free; 13.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ],
      "source": [
        "!python ../reid/main.py data/images/suspects \\\n",
        "    test-detected/output/test \\\n",
        "    test-detected/output/test/image_infos.csv \\\n",
        "    test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVTxEByI1Nu-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "brainhack-od",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}