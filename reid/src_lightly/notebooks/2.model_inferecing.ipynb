{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p2026309/brainhack-til-2023-lazythink/reid/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.data import LightlyDataset\n",
    "\n",
    "from src_lightly.models.dino import DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path: str) -> Image.Image:\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    suspect_dir: str,\n",
    "    output_dir: str,\n",
    "    load_image,\n",
    "    transform,\n",
    "    batch_size = 64,\n",
    "    num_workers = 8\n",
    "):\n",
    "    suspect_dataset = torchvision.datasets.DatasetFolder(\n",
    "        suspect_dir,\n",
    "        load_image,\n",
    "        extensions=[\".png\"],\n",
    "#         transform=transform\n",
    "    )\n",
    "    output_dataset = torchvision.datasets.DatasetFolder(\n",
    "        output_dir,\n",
    "        load_image,\n",
    "        extensions=[\".png\"],\n",
    "#         transform=transform\n",
    "    )\n",
    "    \n",
    "    suspect_dataloader = torch.utils.data.DataLoader(\n",
    "        LightlyDataset.from_torch_dataset(suspect_dataset, transform=transform),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "#         collate_fn = lambda x: tuple(zip(*x)),        \n",
    "    )\n",
    "    \n",
    "    output_dataloader = torch.utils.data.DataLoader(\n",
    "        LightlyDataset.from_torch_dataset(output_dataset, transform=transform),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "#         collate_fn = lambda x: tuple(zip(*x)),        \n",
    "    )\n",
    "\n",
    "    \n",
    "    return suspect_dataloader, output_dataloader\n",
    "    \n",
    "    \n",
    "def load_model(config: dict):\n",
    "    print(\"loading model\")\n",
    "    processor = ViTImageProcessor.from_pretrained('facebook/dino-vitb8')\n",
    "    model = ViTModel.from_pretrained('facebook/dino-vitb8')\n",
    "    print(\"model loaded\")\n",
    "    return model, processor\n",
    "\n",
    "def generate_batch(lst, batch_size):\n",
    "    \"\"\"  Yields batch of specified size \"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i : i + batch_size]\n",
    "\n",
    "\n",
    "def encode_image(model, preprocess, img_paths: List[str], batch_size: int):\n",
    "    print(\"num image found:\", len(img_paths))\n",
    "    with torch.inference_mode(), torch.cuda.amp.autocast():\n",
    "        vectors = []\n",
    "        for img_paths in tqdm.tqdm(generate_batch(img_paths, batch_size)):\n",
    "            inputs = preprocess(\n",
    "                images=[load_image(img_path) for img_path in img_paths],\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            outputs = model(**inputs)\n",
    "            vector = outputs.last_hidden_state\n",
    "            vectors.append(vector.reshape(batch_size, -1).to(\"cpu\"))\n",
    "        vectors = torch.cat(vectors, dim=0)\n",
    "        # normalize vectors prior\n",
    "        vectors = F.normalize(vectors, dim=1)\n",
    "    return vectors\n",
    "\n",
    "def encode_image_dataloader(model, dataloader, preprocess):\n",
    "    vectors = []\n",
    "    with torch.inference_mode(), torch.no_grad():\n",
    "        for imgs, labels in tqdm.tqdm(dataloader):\n",
    "            inputs = preprocess(images=imgs, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            vector = outputs.last_hidden_state\n",
    "            vectors.append(vector.reshape(len(imgs), -1).to(\"cpu\"))\n",
    "        vectors = torch.cat(vectors, dim=0)\n",
    "        # normalize vectors prior\n",
    "        vectors = F.normalize(vectors, dim=1)\n",
    "    return vectors\n",
    "\n",
    "def encode_lightly_dataloader(model, dataloader):\n",
    "    vectors = []\n",
    "    with torch.inference_mode(), torch.no_grad():\n",
    "        for imgs, labels, _ in tqdm.tqdm(dataloader):\n",
    "            vector = model(imgs[0].cuda())\n",
    "            vectors.append(vector.reshape(len(imgs[0]), -1).to(\"cpu\"))\n",
    "        vectors = torch.cat(vectors, dim=0)\n",
    "        # normalize vectors prior\n",
    "        vectors = F.normalize(vectors, dim=1)\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/p2026309/.cache/torch/hub/facebookresearch_dino_main\n",
      "/home/p2026309/brainhack-til-2023-lazythink/reid/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/p2026309/brainhack-til-2023-lazythink/reid/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding suspect images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  7.87it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "suspect_dir = Path(\"data/images/suspects\")\n",
    "cropped_dir = Path(\"test-detected/output/test\")\n",
    "output_csv = Path(\"test.csv\")\n",
    "\n",
    "# load csv\n",
    "df = pd.read_csv(\"test-detected/output/test/image_infos.csv\")\n",
    "df[\"Image_Path\"] = df[\"Image_Name\"].apply(lambda x: str(cropped_dir / x))\n",
    "\n",
    "# load model\n",
    "# model, preprocess = load_model(\n",
    "#     dict(\n",
    "#         model_name=\"ViT-H-14\",\n",
    "#         pretrained=\"laion2b_s32b_b79k\",\n",
    "#         #     jit=True,\n",
    "#         device=DEVICE,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "suspect_dataloader, output_dataloader = load_dataset(\n",
    "    suspect_dir,\n",
    "    cropped_dir,\n",
    "    load_image,\n",
    "    DINOTransform(\n",
    "            hf_prob=0,\n",
    "            cj_prob=0,\n",
    "            random_gray_scale=0,\n",
    "            gaussian_blur=(0, 0, 0),\n",
    "            solarization_prob=0,\n",
    "        ),\n",
    "    64\n",
    ")\n",
    "\n",
    "model = DINO.load_from_checkpoint(\"checkpoints/dino/epoch=79-step=15120.ckpt\",\n",
    "      dataloader_suspect=suspect_dataloader,\n",
    "        suspect_labels=[1,2,3],\n",
    "        num_classes=10,\n",
    "        knn_k=10,\n",
    "        knn_t=0.1,\n",
    " )\n",
    "\n",
    "\n",
    "# encode suspect images\n",
    "print(\"Encoding suspect images\")\n",
    "# suspect_vectors = encode_image(\n",
    "#     model, preprocess, list(suspect_dir.glob(\"*.png\")), 64\n",
    "# )\n",
    "# suspect_vectors = encode_image_dataloader(model, suspect_dataloader, preprocess)\n",
    "\n",
    "suspect_vectors = encode_lightly_dataloader(model, suspect_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding cropped images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:05<00:00,  9.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# encode cropped images\n",
    "print(\"Encoding cropped images\")\n",
    "# cropped_vectors = encode_image_dataloader(model, output_dataloader, preprocess)\n",
    "cropped_vectors = encode_lightly_dataloader(model, output_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity\n",
    "# shape=(num_cropped, num_suspect)\n",
    "similarity_matrix = cropped_vectors.float() @ suspect_vectors.T.float()\n",
    "\n",
    "# find the most similar image\n",
    "max_similarity = torch.max(similarity_matrix, dim=1).values\n",
    "\n",
    "# # check if similarity is greater than threshold\n",
    "# is_suspect = torch.where(max_similarity > args.similarity_threshold, 1, 0)\n",
    "\n",
    "# # save to csv\n",
    "# df[\"class\"] = is_suspect.tolist()\n",
    "# df.to_csv(output_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9863)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_similarity.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
