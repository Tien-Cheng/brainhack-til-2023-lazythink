{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Fine Tuned Whisper Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "Collecting librosa\n",
      "  Using cached librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
      "Collecting jiwer\n",
      "  Using cached jiwer-3.0.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.5.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.10.6)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.3.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2023.1.0)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from librosa) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from librosa) (1.7.3)\n",
      "Collecting pooch<1.7,>=1.0\n",
      "  Using cached pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "Collecting audioread>=2.1.9\n",
      "  Using cached audioread-3.0.0-py3-none-any.whl\n",
      "Collecting soundfile>=0.12.1\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.7/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.7/site-packages (from librosa) (0.56.4)\n",
      "Collecting lazy-loader>=0.1\n",
      "  Using cached lazy_loader-0.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.7/site-packages (from librosa) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from librosa) (4.5.0)\n",
      "Collecting soxr>=0.3.2\n",
      "  Using cached soxr-0.3.5-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.7/site-packages (from jiwer) (8.1.3)\n",
      "Collecting rapidfuzz==2.13.7\n",
      "  Using cached rapidfuzz-2.13.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba>=0.51.0->librosa) (67.6.0)\n",
      "Collecting appdirs>=1.3.0\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.7/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2023.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: tokenizers, appdirs, xxhash, soxr, regex, rapidfuzz, lazy-loader, dill, audioread, soundfile, responses, pooch, multiprocess, huggingface-hub, transformers, librosa, jiwer, datasets, evaluate\n",
      "Successfully installed appdirs-1.4.4 audioread-3.0.0 datasets-2.12.0 dill-0.3.6 evaluate-0.4.0 huggingface-hub-0.14.1 jiwer-3.0.1 lazy-loader-0.2 librosa-0.10.0.post2 multiprocess-0.70.14 pooch-1.6.0 rapidfuzz-2.13.7 regex-2023.5.5 responses-0.18.0 soundfile-0.12.1 soxr-0.3.5 tokenizers-0.13.3 transformers-4.29.2 xxhash-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Run this if required\n",
    "%pip install transformers evaluate datasets librosa jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load model and processor\n",
    "MODEL_PATH = \"<path-to-model>\"\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_PATH)\n",
    "model = Whiprocessor = WhisperProcessor.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"gs://cloud-ai-platform-e8edc327-855c-4911-bb8e-205517f8c899/asr/data/train/til_asr_base_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ds = load_from_disk(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'audio/train_03701.wav',\n",
       " 'annotation': 'THERE IS ONLY ONE WAY TO SUCCESS AND THAT IS HARD WORK AND DETERMINATION',\n",
       " 'audio': {'path': 'train_03701.wav',\n",
       "  'array': array([-3.96728516e-04, -5.18798828e-04, -4.88281250e-04, ...,\n",
       "         -3.05175781e-05, -3.05175781e-05, -9.15527344e-05]),\n",
       "  'sampling_rate': 16000}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['path', 'annotation', 'audio'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['path', 'annotation', 'audio'],\n",
       "        num_rows: 750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py:1350: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There is only one way to success and that is hard work and determination.\n"
     ]
    }
   ],
   "source": [
    "input_speech = ds[\"train\"][0][\"audio\"]\n",
    "input_features = processor(\n",
    "    input_speech[\"array\"],\n",
    "    sampling_rate=input_speech[\"sampling_rate\"],\n",
    "    return_tensors=\"pt\"\n",
    ").input_features\n",
    "\n",
    "# generate\n",
    "generated_ids = model.generate(\n",
    "    input_features.to(DEVICE),\n",
    "    forced_decoder_ids=forced_decoder_ids,\n",
    ")\n",
    "\n",
    "# decode to text\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_to_til(transcript: str) -> str:\n",
    "    # TIL output is purely uppercase alphabet and space\n",
    "    # so we normalize the output to that\n",
    "    result = \"\".join([c.upper() if c.isalpha() else \" \" for c in transcript])\n",
    "    # Remove double spaces\n",
    "    while \"  \" in result:\n",
    "        result = result.replace(\"  \", \" \")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5466b4dce3bd429faf4325f8b92f613a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train WER: 3.665\n",
      "Test WER: 3.489\n"
     ]
    }
   ],
   "source": [
    "def map_to_pred(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    raw = [i[\"array\"] for i in audio]\n",
    "    \n",
    "    input_features = processor(raw, sampling_rate=audio[0][\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "    batch[\"reference\"] = [normalize_to_til(processor.tokenizer._normalize(transcript)) for transcript in batch[\"annotation\"]]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features.to(DEVICE))\n",
    "    preds = []\n",
    "    for pred in predicted_ids:\n",
    "      transcription = processor.decode(pred)\n",
    "      preds.append(normalize_to_til(processor.tokenizer._normalize(transcription)))\n",
    "    batch[\"prediction\"] = preds\n",
    "    return batch\n",
    "\n",
    "preds = ds.map(map_to_pred, batched=True, batch_size=32)\n",
    "\n",
    "# Calculate WER\n",
    "wer = evaluate.load(\"wer\")\n",
    "test_result = wer.compute(predictions=preds[\"test\"][\"prediction\"], references=preds[\"test\"][\"reference\"])\n",
    "train_result = wer.compute(predictions=preds[\"train\"][\"prediction\"], references=preds[\"train\"][\"reference\"])\n",
    "print(f\"Train WER: {train_result * 100:.3f}\")\n",
    "print(f\"Test WER: {test_result * 100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def map_to_pred_eval(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    raw = [i[\"array\"] for i in audio]\n",
    "    \n",
    "    input_features = processor(raw, sampling_rate=audio[0][\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features.to(DEVICE))\n",
    "    preds = []\n",
    "    for pred in predicted_ids:\n",
    "      transcription = processor.decode(pred)\n",
    "      preds.append(normalize_to_til(processor.tokenizer._normalize(transcription)))\n",
    "    batch[\"prediction\"] = preds\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVAL_DATASET = \"gs://cloud-ai-platform-e8edc327-855c-4911-bb8e-205517f8c899/asr/data/test/til_asr_base_eval\"\n",
    "eval_ds = load_from_disk(EVAL_DATASET)\n",
    "eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1567581b378e4462b26611ffb1e99d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_preds = eval_ds.map(map_to_pred_eval, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def create_csv(paths, predictions, output_filename):\n",
    "    # Ensure that paths and predictions are of the same length\n",
    "    assert len(paths) == len(predictions), \"Lists must have the same length\"\n",
    "\n",
    "    with open(output_filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['path', 'annotation']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for path, prediction in zip(paths, predictions):\n",
    "            writer.writerow({'path': path[6:], 'annotation': prediction})\n",
    "\n",
    "# Test with example data\n",
    "paths = eval_ds[\"path\"]\n",
    "predictions = eval_ds[\"prediction\"]\n",
    "\n",
    "create_csv(paths, predictions, 'eval_submission_whisper_small_zero_shot_defaultdecoding.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Error Analysis\n",
    "- Denoising of Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
